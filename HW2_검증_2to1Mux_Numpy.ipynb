{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2to1Mux_Numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boldasl/20_IT_SYSTEM_DESIGN_008/blob/master/HW2_%EA%B2%80%EC%A6%9D_2to1Mux_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UCgWAXLrRgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training 2to1 Mux using only numpy\n",
        "\n",
        "import numpy as np\n",
        "from math import exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from timeit import default_timer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiiXTRHGreyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    activation = 0 # activation function\n",
        "    variance = 0.1 # variance for weight initiation\n",
        "\n",
        "    num_inputnode = 0 # the number of input node\n",
        "    num_outputnode = 0 # the number of output node\n",
        "    num_hiddenlayer = 0 # the number of hidden layer\n",
        "    num_hiddennode = 0 # the number of hidden node\n",
        "\n",
        "    w_inputnode = 0 # weight for input layer to hidden_0 layer\n",
        "    w_hiddennode = 0 # weight for hidden_0 layer to hidden_n layer\n",
        "    w_outputnode = 0 # weight for hidden_n layer to output layer\n",
        "\n",
        "    b_hiddennode = 0 # bias for hidden layer\n",
        "    b_outputnode = 0 # bias for output layer\n",
        "\n",
        "    d_hiddennode = 0 # delta for hidden layer\n",
        "    d_outputnode = 0 # delta for output layer\n",
        "\n",
        "    o_hidden = 0 # output for hidden layer\n",
        "    o_output = 0 # output for output layer\n",
        "\n",
        "    data_train = 0 # training data\n",
        "    label_train = 0 # training label\n",
        "    data_test = 0 # test data\n",
        "\n",
        "    error = 0 # cost function\n",
        "\n",
        "    def __init__(self, inputnode, outputnode, hiddenlayer, hiddennode, act):\n",
        "        self.num_inputnode = inputnode\n",
        "        self.num_outputnode = outputnode\n",
        "        self.num_hiddenlayer = hiddenlayer\n",
        "        self.num_hiddennode = hiddennode\n",
        "        self.activation = act\n",
        "\n",
        "        # weight & bias init\n",
        "        self.w_inputnode = np.random.normal(0, self.variance, size=[self.num_hiddennode, self.num_inputnode])\n",
        "        if self.num_hiddenlayer > 1:\n",
        "            self.w_hiddennode = np.random.normal(0, self.variance, size=[self.num_hiddenlayer-1, self.num_hiddennode, self.num_hiddennode])\n",
        "        self.w_outputnode = np.random.normal(0, self.variance, size=[self.num_outputnode, self.num_hiddennode])\n",
        "\n",
        "        self.b_hiddennode = np.random.normal(0, self.variance, size=[self.num_hiddenlayer, self.num_hiddennode])\n",
        "        self.b_outputnode = np.random.normal(0, self.variance, size=self.num_outputnode)\n",
        "\n",
        "        # delta init\n",
        "        self.d_hiddennode = np.zeros([self.num_hiddenlayer, self.num_hiddennode])\n",
        "        self.d_outputnode = np.zeros(self.num_outputnode)\n",
        "\n",
        "        # output init\n",
        "        self.o_hidden = np.zeros([self.num_hiddenlayer, self.num_hiddennode])\n",
        "        self.o_output = np.zeros(self.num_outputnode)\n",
        "\n",
        "        #np.random.seed(0) # seed\n",
        "\n",
        "        print(\"< Neural Network Using Numpy >\")\n",
        "        print(\"# of InputNode  : {}   # of OutputNode: {}\".format(self.num_inputnode, self.num_outputnode))\n",
        "        print(\"# of HiddenLayer: {}   # of HiddenNode: {}\".format(self.num_hiddenlayer, self.num_hiddennode))\n",
        "        print(\"Activation Function: {}\\n\".format(self.activation))\n",
        "\n",
        "    def input_dataset(self, train_data, train_label, test_data):\n",
        "        self.data_train = train_data\n",
        "        self.label_train = train_label\n",
        "        self.data_test = test_data\n",
        "\n",
        "    # actication function\n",
        "    def func_act(self, a):\n",
        "        len_a = len(a)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            return np.array([1 / (1 + exp(-a[i])) for i in range(len_a)])\n",
        "        elif self.activation == \"relu\":\n",
        "            return np.array([a[i] if a[i] > 0 else 0 for i in range(len_a)])\n",
        "        elif self.activation == \"tanh\":\n",
        "            return np.array([np.tanh(a[i]) for i in range(len_a)])\n",
        "        else: print(\"Error::NeuralNetwork.func_act;    Function Name\")\n",
        "\n",
        "    # actication function derivative\n",
        "    def func_act_d(self, y):\n",
        "        len_y = len(y)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            return np.array([y[i]*(1-y[i]) for i in range(len_y)])\n",
        "        elif self.activation == \"relu\":\n",
        "            return np.array([1 if y[i] > 0 else 0 for i in range(len_y)])\n",
        "        elif self.activation == \"tanh\":\n",
        "            return np.array([(1-y[i])*(1+y[i]) for i in range(len_y)])\n",
        "        else: print(\"Error::NeuralNetwork.func_act_d;    Function Name\")\n",
        "\n",
        "    # mean square error function\n",
        "    def func_mse(self, y, y_h):\n",
        "        return sum(abs(y-y_h))\n",
        "\n",
        "    # mean square error functon derivative\n",
        "    def func_mse_d(self, y, y_h):\n",
        "        len_y = len(y)\n",
        "        if len_y != len(y_h): print(\"Error:NeuralNetwork.func_mse_d;    Vector Length\")\n",
        "        return np.array([y[i]-y_h[i] for i in range(len_y)])\n",
        "\n",
        "    def func_threshold(self, x):\n",
        "        len_x = len(x)\n",
        "        if self.activation == \"tanh\": result = [1 if x[i] > 0 else -1 for i in range(len_x)]\n",
        "        else: result = [1 if x[i] > 0.5 else 0 for i in range(len_x)]\n",
        "\n",
        "        return np.array(result)\n",
        "\n",
        "    def perceptron(self, x, w, b):\n",
        "        a = x @ w.T + b # w = [layer][node_after][node_before]\n",
        "        return self.func_act(a)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        self.o_hidden[0] = self.perceptron(x, self.w_inputnode, self.b_hiddennode[0])\n",
        "        if self.num_hiddenlayer > 1:\n",
        "            for idx_layer in range(self.num_hiddenlayer-1):\n",
        "                self.o_hidden[idx_layer+1] = self.perceptron(self.o_hidden[idx_layer], self.w_hiddennode[idx_layer], self.b_hiddennode[idx_layer+1])\n",
        "        self.o_output = self.perceptron(self.o_hidden[self.num_hiddenlayer-1], self.w_outputnode, self.b_outputnode)\n",
        "        return self.o_output\n",
        "\n",
        "    def backpropagation(self, y, y_h):\n",
        "        self.d_outputnode = self.func_act_d(y_h) * self.func_mse_d(y, y_h)\n",
        "\n",
        "        if self.num_hiddenlayer > 1:\n",
        "            self.d_hiddennode[self.num_hiddenlayer-1] = self.func_act_d(self.o_hidden[self.num_hiddenlayer-1]) * (self.d_outputnode @ self.w_outputnode)\n",
        "            for idx_layer in range(self.num_hiddenlayer-2, -1, -1):\n",
        "                self.d_hiddennode[idx_layer] = self.func_act_d(self.o_hidden[idx_layer]) * (self.d_hiddennode[idx_layer+1] @ self.w_hiddennode[idx_layer])\n",
        "        else:\n",
        "            self.d_hiddennode[0] = self.func_act_d(self.o_hidden[0]) * (self.d_outputnode @ self.w_outputnode)\n",
        "\n",
        "    def learn(self, epoch, lr):\n",
        "        print(\"Epoch: {}, Learning Rate: {}\".format(epoch, lr))\n",
        "        print(\"Learning Start!\")\n",
        "        self.error = []\n",
        "        for idx_epoch in range(epoch):\n",
        "            error_temp = 0\n",
        "            for idx_test in range(self.data_test.shape[0]):\n",
        "                expected = self.feedforward(self.data_train[idx_test])\n",
        "                self.backpropagation(self.label_train[idx_test], expected)\n",
        "                error_temp += self.func_mse(self.label_train[idx_test], expected)\n",
        "\n",
        "                u_w_inputnode = np.outer(self.d_hiddennode[0], self.data_train[idx_test]) # update value\n",
        "                self.w_inputnode = self.w_inputnode + lr * u_w_inputnode\n",
        "                self.b_hiddennode[0] = self.b_hiddennode[0] + lr * self.d_hiddennode[0]\n",
        "\n",
        "                if self.num_hiddenlayer > 1:\n",
        "                    u_w_hiddennode = np.zeros(self.w_hiddennode.shape)\n",
        "                    for idx_layer in range(self.num_hiddenlayer-1):\n",
        "                        u_w_hiddennode[idx_layer] = np.outer(self.d_hiddennode[idx_layer+1], self.o_hidden[idx_layer]) # update value\n",
        "                    u_w_outputnode = np.outer(self.d_outputnode, self.o_hidden[self.num_hiddenlayer-1]) # update value\n",
        "                    self.w_hiddennode = self.w_hiddennode + lr * u_w_hiddennode\n",
        "                else:\n",
        "                    u_w_outputnode = np.outer(self.d_outputnode, self.o_hidden[0]) # update value\n",
        "\n",
        "                self.w_outputnode = self.w_outputnode + lr * u_w_outputnode\n",
        "                self.b_outputnode = self.b_outputnode + lr * self.d_outputnode\n",
        "            self.error.append(error_temp / self.data_test.shape[0])\n",
        "            if idx_epoch % 10 == 0:\n",
        "                print(\"{0:0.1f}%, Cost = {1:0.4f}\\r\".format(idx_epoch/epoch*100, error_temp/self.data_test.shape[0]), end='')\n",
        "        print(\"100.0%, Cost = {0:0.4f}\".format(error_temp/self.data_test.shape[0]))\n",
        "\n",
        "    def evaluation(self, repeat, variance):\n",
        "        print(\"Evaluation Start!\")\n",
        "        accuracy = []\n",
        "        for idx_repeat in range(repeat):\n",
        "            test_input = self.data_test + np.random.normal(0, variance, size=self.data_test.shape)\n",
        "\n",
        "            for idx_test in range(self.data_test.shape[0]):\n",
        "                expected = self.func_threshold(self.feedforward(test_input[idx_test]))\n",
        "                correct = self.label_train[idx_test]\n",
        "                \n",
        "                if sum(abs(expected - correct)) == 0: accuracy.append(1) # if this test case is correct\n",
        "                else: accuracy.append(0) # if this test case isn't correct\n",
        "        accuracy = np.average(accuracy) # average\n",
        "        print(\"Accuracy: {0:0.4f}\\t(test variance = {1})\\n\".format(accuracy, variance))\n",
        "\n",
        "    def plot(self):\n",
        "        epoch = len(self.error)\n",
        "        plt.plot(range(epoch), self.error)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xlim(0, epoch)\n",
        "        plt.ylim(0, 1.2)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jFnwgUIrkMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning configuration\n",
        "LearningRate = 0.1\n",
        "Epoch = 1000\n",
        "Activation = \"tanh\"\n",
        "NumInputNode = 3\n",
        "NumOutputNode = 1\n",
        "NumHiddenLayer = 1\n",
        "NumHiddenNode = 3\n",
        "\n",
        "# train data set: 2to1 Mux\n",
        "# train_data = [select, in_1, in_0]\n",
        "# train_label = [out]\n",
        "train_data = [[-1, -1, -1], [-1, -1, 1], [-1, 1, -1], [-1, 1, 1], [1, -1, -1], [1, -1, 1], [1, 1, -1], [1, 1, 1]] \n",
        "train_label = [[-1], [1], [-1], [1], [-1], [-1], [1], [1]] \n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_label = np.array(train_label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmaP7JOar0jQ",
        "colab_type": "code",
        "outputId": "620a0b98-6205-4117-c609-e10470b3a087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "model = NeuralNetwork(NumInputNode, NumOutputNode, NumHiddenLayer, NumHiddenNode, Activation)\n",
        "model.input_dataset(train_data, train_label, train_data) # training data, training label, test data\n",
        "\n",
        "before = default_timer()\n",
        "model.learn(epoch=Epoch, lr=LearningRate)\n",
        "print(\"Training Time = {0:0.2f}[s]\\n\".format(default_timer()-before))\n",
        "model.evaluation(repeat=100, variance=0.1)\n",
        "model.plot()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< Neural Network Using Numpy >\n",
            "# of InputNode  : 3   # of OutputNode: 1\n",
            "# of HiddenLayer: 1   # of HiddenNode: 3\n",
            "Activation Function: tanh\n",
            "\n",
            "Epoch: 1000, Learning Rate: 0.1\n",
            "Learning Start!\n",
            "100.0%, Cost = 0.0171\n",
            "Training Time = 1.45[s]\n",
            "\n",
            "Evaluation Start!\n",
            "Accuracy: 1.0000\t(test variance = 0.1)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}