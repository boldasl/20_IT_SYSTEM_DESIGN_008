{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FA_Numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boldasl/20_IT_SYSTEM_DESIGN_008/blob/master/FA_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UCgWAXLrRgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from math import exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import timeit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiiXTRHGreyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    activation = 0 # activation function\n",
        "    variance = 0.1 # variance for weight initiation\n",
        "\n",
        "    num_inputnode = 0 # the number of input node\n",
        "    num_outputnode = 0 # the number of output node\n",
        "    num_hiddenlayer = 0 # the number of hidden layer\n",
        "    num_hiddennode = 0 # the number of hidden node\n",
        "\n",
        "    w_inputnode = 0 # weight for input layer to hidden_0 layer\n",
        "    w_hiddennode = 0 # weight for hidden_0 layer to hidden_n layer\n",
        "    w_outputnode = 0 # weight for hidden_n layer to output layer\n",
        "\n",
        "    b_hiddennode = 0 # bias for hidden layer\n",
        "    b_outputnode = 0 # bias for output layer\n",
        "\n",
        "    d_hiddennode = 0 # delta for hidden layer\n",
        "    d_outputnode = 0 # delta for output layer\n",
        "\n",
        "    o_hidden = 0 # output for hidden layer\n",
        "    o_output = 0 # output for output layer\n",
        "\n",
        "    data_train = 0 # training data\n",
        "    label_train = 0 # training label\n",
        "    data_test = 0 # test data\n",
        "\n",
        "    error = 0 # cost function\n",
        "\n",
        "    def __init__(self, inputnode, outputnode, hiddenlayer, hiddennode, act):\n",
        "        self.num_inputnode = inputnode\n",
        "        self.num_outputnode = outputnode\n",
        "        self.num_hiddenlayer = hiddenlayer\n",
        "        self.num_hiddennode = hiddennode\n",
        "        self.activation = act\n",
        "\n",
        "        # weight & bias init\n",
        "        self.w_inputnode = np.random.normal(0, self.variance, size=[self.num_hiddennode, self.num_inputnode])\n",
        "        if self.num_hiddenlayer > 1:\n",
        "            self.w_hiddennode = np.random.normal(0, self.variance, size=[self.num_hiddenlayer-1, self.num_hiddennode, self.num_hiddennode])\n",
        "        self.w_outputnode = np.random.normal(0, self.variance, size=[self.num_outputnode, self.num_hiddennode])\n",
        "\n",
        "        self.b_hiddennode = np.random.normal(0, self.variance, size=[self.num_hiddenlayer, self.num_hiddennode])\n",
        "        self.b_outputnode = np.random.normal(0, self.variance, size=self.num_outputnode)\n",
        "\n",
        "        # delta init\n",
        "        self.d_hiddennode = np.zeros([self.num_hiddenlayer, self.num_hiddennode])\n",
        "        self.d_outputnode = np.zeros(self.num_outputnode)\n",
        "\n",
        "        # output init\n",
        "        self.o_hidden = np.zeros([self.num_hiddenlayer, self.num_hiddennode])\n",
        "        self.o_output = np.zeros(self.num_outputnode)\n",
        "\n",
        "        #np.random.seed(0) # seed\n",
        "\n",
        "        print(\"< Neural Network Using Numpy >\")\n",
        "        print(\"# of InputNode  : {}   # of OutputNode: {}\".format(self.num_inputnode, self.num_outputnode))\n",
        "        print(\"# of HiddenLayer: {}   # of HiddenNode: {}\".format(self.num_hiddenlayer, self.num_hiddennode))\n",
        "        print(\"Activation Function: {}\\n\".format(self.activation))\n",
        "\n",
        "    def input_dataset(self, train_data, train_label, test_data):\n",
        "        self.data_train = train_data\n",
        "        self.label_train = train_label\n",
        "        self.data_test = test_data\n",
        "\n",
        "    # actication function\n",
        "    def func_act(self, a):\n",
        "        len_a = len(a)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            return np.array([1 / (1 + exp(-a[i])) for i in range(len_a)])\n",
        "        elif self.activation == \"relu\":\n",
        "            return np.array([a[i] if a[i] > 0 else 0 for i in range(len_a)])\n",
        "        elif self.activation == \"tanh\":\n",
        "            return np.array([np.tanh(a[i]) for i in range(len_a)])\n",
        "        else: print(\"Error::NeuralNetwork.func_act;    Function Name\")\n",
        "\n",
        "    # actication function derivative\n",
        "    def func_act_d(self, y):\n",
        "        len_y = len(y)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            return np.array([y[i]*(1-y[i]) for i in range(len_y)])\n",
        "        elif self.activation == \"relu\":\n",
        "            return np.array([1 if y[i] > 0 else 0 for i in range(len_y)])\n",
        "        elif self.activation == \"tanh\":\n",
        "            return np.array([(1-y[i])*(1+y[i]) for i in range(len_y)])\n",
        "        else: print(\"Error::NeuralNetwork.func_act_d;    Function Name\")\n",
        "\n",
        "    # mean square error function\n",
        "    def func_mse(self, y, y_h):\n",
        "        return sum(abs(y-y_h))\n",
        "\n",
        "    # mean square error functon derivative\n",
        "    def func_mse_d(self, y, y_h):\n",
        "        len_y = len(y)\n",
        "        if len_y != len(y_h): print(\"Error:NeuralNetwork.func_mse_d;    Vector Length\")\n",
        "        return np.array([y[i]-y_h[i] for i in range(len_y)])\n",
        "\n",
        "    def func_threshold(self, x):\n",
        "        len_x = len(x)\n",
        "        if self.activation == \"tanh\": result = [1 if x[i] > 0 else -1 for i in range(len_x)]\n",
        "        else: result = [1 if x[i] > 0.5 else 0 for i in range(len_x)]\n",
        "\n",
        "        return np.array(result)\n",
        "\n",
        "    def perceptron(self, x, w, b):\n",
        "        a = x @ w.T + b # w = [layer][node_after][node_before]\n",
        "        return self.func_act(a)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        self.o_hidden[0] = self.perceptron(x, self.w_inputnode, self.b_hiddennode[0])\n",
        "        if self.num_hiddenlayer > 1:\n",
        "            for idx_layer in range(self.num_hiddenlayer-1):\n",
        "                self.o_hidden[idx_layer+1] = self.perceptron(self.o_hidden[idx_layer], self.w_hiddennode[idx_layer], self.b_hiddennode[idx_layer+1])\n",
        "        self.o_output = self.perceptron(self.o_hidden[self.num_hiddenlayer-1], self.w_outputnode, self.b_outputnode)\n",
        "        return self.o_output\n",
        "\n",
        "    def backpropagation(self, y, y_h):\n",
        "        self.d_outputnode = self.func_act_d(y_h) * self.func_mse_d(y, y_h)\n",
        "\n",
        "        if self.num_hiddenlayer > 1:\n",
        "            self.d_hiddennode[self.num_hiddenlayer-1] = self.func_act_d(self.o_hidden[self.num_hiddenlayer-1]) * (self.d_outputnode @ self.w_outputnode)\n",
        "            for idx_layer in range(self.num_hiddenlayer-2, -1, -1):\n",
        "                self.d_hiddennode[idx_layer] = self.func_act_d(self.o_hidden[idx_layer]) * (self.d_hiddennode[idx_layer+1] @ self.w_hiddennode[idx_layer])\n",
        "        else:\n",
        "            self.d_hiddennode[0] = self.func_act_d(self.o_hidden[0]) * (self.d_outputnode @ self.w_outputnode)\n",
        "\n",
        "    def learn(self, epoch, lr):\n",
        "        print(\"Epoch: {}, Learning Rate: {}\".format(epoch, lr))\n",
        "        print(\"Learning Start!\")\n",
        "        self.error = []\n",
        "        for idx_epoch in range(epoch):\n",
        "            error_temp = 0\n",
        "            for idx_test in range(self.data_test.shape[0]):\n",
        "                expected = self.feedforward(self.data_train[idx_test])\n",
        "                self.backpropagation(self.label_train[idx_test], expected)\n",
        "                error_temp += self.func_mse(self.label_train[idx_test], expected)\n",
        "\n",
        "                u_w_inputnode = np.outer(self.d_hiddennode[0], self.data_train[idx_test]) # update value\n",
        "                self.w_inputnode = self.w_inputnode + lr * u_w_inputnode\n",
        "                self.b_hiddennode[0] = self.b_hiddennode[0] + lr * self.d_hiddennode[0]\n",
        "\n",
        "                if self.num_hiddenlayer > 1:\n",
        "                    u_w_hiddennode = np.zeros(self.w_hiddennode.shape)\n",
        "                    for idx_layer in range(self.num_hiddenlayer-1):\n",
        "                        u_w_hiddennode[idx_layer] = np.outer(self.d_hiddennode[idx_layer+1], self.o_hidden[idx_layer]) # update value\n",
        "                    u_w_outputnode = np.outer(self.d_outputnode, self.o_hidden[self.num_hiddenlayer-1]) # update value\n",
        "                    self.w_hiddennode = self.w_hiddennode + lr * u_w_hiddennode\n",
        "                else:\n",
        "                    u_w_outputnode = np.outer(self.d_outputnode, self.o_hidden[0]) # update value\n",
        "\n",
        "                self.w_outputnode = self.w_outputnode + lr * u_w_outputnode\n",
        "                self.b_outputnode = self.b_outputnode + lr * self.d_outputnode\n",
        "            self.error.append(error_temp / self.data_test.shape[0])\n",
        "            if idx_epoch % 10 == 0:\n",
        "                print(\"{0:0.1f}%, Cost = {1:0.4f}\\r\".format(idx_epoch/epoch*100, error_temp/self.data_test.shape[0]), end='')\n",
        "        print(\"100.0%, Cost = {0:0.4f}\\n\".format(error_temp/self.data_test.shape[0]))\n",
        "\n",
        "    def evaluation(self, repeat, variance):\n",
        "        print(\"Evaluation Start!\")\n",
        "        accuracy = []\n",
        "        for idx_repeat in range(repeat):\n",
        "            test_input = self.data_test + np.random.normal(0, variance, size=self.data_test.shape)\n",
        "\n",
        "            for idx_test in range(self.data_test.shape[0]):\n",
        "                expected = self.func_threshold(self.feedforward(test_input[idx_test]))\n",
        "                correct = self.label_train[idx_test]\n",
        "                \n",
        "                if sum(abs(expected - correct)) == 0: accuracy.append(1) # if this test case is correct\n",
        "                else: accuracy.append(0) # if this test case isn't correct\n",
        "        accuracy = np.average(accuracy) # average\n",
        "        print(\"Accuracy: {0:0.4f}\\t(test variance = {1})\\n\".format(accuracy, variance))\n",
        "\n",
        "    def plot(self):\n",
        "        epoch = len(self.error)\n",
        "        plt.plot(range(epoch), self.error)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xlim(0, epoch)\n",
        "        plt.ylim(0, 1.2)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jFnwgUIrkMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning configuration\n",
        "LearningRate = 0.3\n",
        "Epoch = 10000\n",
        "Activation = \"tanh\"\n",
        "NumInputNode = 3\n",
        "NumOutputNode = 2\n",
        "NumHiddenLayer = 1\n",
        "NumHiddenNode = 3\n",
        "\n",
        "# train data set\n",
        "train_data = [[-1, -1, -1], [-1, -1, 1], [-1, 1, -1], [-1, 1, 1], [1, -1, -1], [1, -1, 1], [1, 1, -1], [1, 1, 1]] # test input = [a, b, cin]\n",
        "train_label = [[-1, -1], [1, -1], [1, -1], [-1, 1], [1, -1], [-1, 1], [-1, 1], [1, 1]] # ground truth value = [sum, cout]\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_label = np.array(train_label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmaP7JOar0jQ",
        "colab_type": "code",
        "outputId": "420fae77-e4ab-4613-d70f-ec61df988f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "nn = NeuralNetwork(NumInputNode, NumOutputNode, NumHiddenLayer, NumHiddenNode, Activation)\n",
        "nn.input_dataset(train_data, train_label, train_data) # training data, training label, test data\n",
        "\n",
        "w_i = nn.w_inputnode\n",
        "w_o = nn.w_outputnode\n",
        "\n",
        "b_h = nn.b_hiddennode\n",
        "b_o = nn.b_outputnode\n",
        "before = timeit.default_timer()\n",
        "nn.learn(epoch=Epoch, lr=LearningRate)\n",
        "after = timeit.default_timer()\n",
        "print(after-before, \"[s]\")\n",
        "for _ in range(1): nn.evaluation(repeat=100, variance=0.1)\n",
        "nn.plot()\n",
        "\n",
        "w_i = abs(w_i - nn.w_inputnode)\n",
        "w_o = abs(w_o - nn.w_outputnode)\n",
        "b_h = abs(b_h - nn.b_hiddennode)\n",
        "b_o = abs(b_o - nn.b_outputnode)\n",
        "'''\n",
        "print(np.sum(w_i))\n",
        "print(np.sum(w_o))\n",
        "print(np.sum(b_h))\n",
        "print(np.sum(b_o))\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "< Neural Network Using Numpy >\n",
            "# of InputNode  : 3   # of OutputNode: 2\n",
            "# of HiddenLayer: 1   # of HiddenNode: 3\n",
            "Activation Function: tanh\n",
            "\n",
            "Epoch: 10000, Learning Rate: 0.3\n",
            "Learning Start!\n",
            "100.0%, Cost = 0.0059\n",
            "\n",
            "11.935076586000008 [s]\n",
            "Evaluation Start!\n",
            "Accuracy: 1.0000\t(test variance = 0.1)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(np.sum(w_i))\\nprint(np.sum(w_o))\\nprint(np.sum(b_h))\\nprint(np.sum(b_o))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}